Processor analysis

Ming's Linux computer uses 4th Gen i7 4720HQ processor, based off of Haswell microarchitecture.
L1 cache 64KB per core, L2 cache 256 KB per core, L3 cache 6MB shared

4 ALUs, 3 Address Generation Units
https://en.wikipedia.org/wiki/Haswell_(microarchitecture)

Fortran uses Col;umn-Major storage order:
http://h21007.www2.hp.com/portal/download/files/unprot/fortran/docs/vf-html/pg/pguaracc.htm



BT Analysis:


Loop was nested so that the fastest changing variable is the leftmost to ensure unit stride.
This was not often enforced and so were more cache misses. This can be found in most components
of the BT benchmark.

The equations are very long and thus create data dependence. There is also a low number
of arithmetic units. Lots of array elements are used and thus there are many data memory
accesses.

High memory requirements. Problem size scaled cubicly, given the stencil was performed on 3D
matrices. There were also many auxilliary matricies used to calculated other terms used in
the CFD simulations. Given the number of floating point operations and terms required, it
used up cache memory. We see drop in performance as number of cores increased because of
restricted cache size.

**The above points are prominent in xsolve, ysolve, and zsolve. Appears also in rhs*

True data dependencies in rhs& subroutine since many terms depend on terms calculated
in previous iterations of the loop.



LU Analysis:

Loop was nested so that the fastest changing variable is the leftmost to ensure unit stride.
This was not often enforced and so were more cache misses. This can be found in most components
of the LE benchmark.

The equations are very long and thus create data dependence. There is also a low number
of arithmetic units. Lots of array elements are used and thus there are many data memory
accesses.

High memory requirements. Problem size scaled cubicly, given the stencil was performed on 3D
matrices. There were also many auxilliary matricies used to calculated other terms used in
the CFD simulations. Given the number of floating point operations and terms required, it
used up cache memory. We see drop in performance as number of cores increased because of
restricted cache size.

**The above two points are prominent in jacld, blts, jacu, buts. Appears also in rhs*

True data dependencies in rhs subroutine since many terms depend on terms calculated
in previous iterations of the loop.



SP Analysis:

Loop was nested so that the fastest changing variable is the leftmost to ensure unit stride.
This was not often enforced and so were more cache misses. This can be found in most components
of the SP benchmark.

The equations are very long and thus create data dependence. There is also a low number
of arithmetic units. Lots of array elements are used and thus there are many data memory
accesses.

High memory requirements. Problem size scaled cubicly, given the stencil was performed on 3D
matrices. There were also many auxilliary matricies used to calculated other terms used in
the CFD simulations. Given the number of floating point operations and terms required, it
used up cache memory. We see drop in performance as number of cores increased because of
restricted cache size.

**The above two points are prominent in rhs*, xsolve, ysolve, zsolve, . Appears also in rhs*

True data dependencies in rhs& subroutine since many terms depend on terms calculated
in previous iterations of the loop.

